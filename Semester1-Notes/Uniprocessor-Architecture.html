<!DOCTYPE html>
<html>
	<head>
		<title>Uniprocessor Architecture Notes</title>
		<link rel="stylesheet" href="../css/semester-notes-style.css"/>
	</head>
	<body>
		<main>
			<h1>Uniprocessor architecture</h1>
			<p>
				The most common uniprocessor architecture is the Von Neumann architecture. These architectures can only do 1 calculation at a time (as opposed to parrallel architectures). Computer architectures are abstractions (generalisations) that allow programs to be run on different types of machines.
			</p>
			<section>
				<h2>Von Neumann Architecture</h2>
				<p>
					The Von Neumann architecture has 3 main components - the <abbr title="Arithmetic Logic Unit">ALU</abbr>, the Control Unit, and the Memory Store. The <abbr>ALU</abbr> is where data is processed. To process the data, operations (instructions e.g. add) and operands (e.g. numbers) are needed. The <abbr>ALU</abbr> is controlled by the Control Unit - which selects and feeds in the information to the <abbr>ALU</abbr> at the correct time. The Control Unit knows when to send information to the <abbr>ALU</abbr> as it knows what the <abbr>ALU</abbr>'s state is as the <abbr>ALU</abbr> feeds this information back to the Control Unit. Together, the <abbr>ALU</abbr> and Control Unit form the <abbr title="Central Processing Unit">CPU</abbr>.
				</p>
				<p>
					Data is stored in memory. The big insight into Von Neumann architecture is that the data for instructions and the data for operands are stored in the same location. The Control Unit prompts the memory to send back operations (Fetch) (which is then sent to the <abbr>ALU</abbr> via the Control Lines (Decode and Execute)) and also prompts the memory to send operands directly to the <abbr>ALU</abbr>. The new processed data from the <abbr>ALU</abbr> is then sent back into the memory. If the data store and control unit work fast enough, the <abbr>ALU</abbr> can be kept in continuous operation. There are 2 types of memory store, sequential (e.g. magnetic tape) and random (e.g. <abbr title="Random Access Memory">RAM</abbr>. The time to access the next address with sequential memory depends on the distance between the previous memory address and the current one (as the tape must rewind or fast forward) while for <abbr>RAM</abbr> the time does not depend on the previous fetch cycle.
				</p>
				<p>
					Humans can interface with the computer using input and output devices or <abbr title="Input Output">IO</abbr>. These are connected to the Control Unit.
				</p>
				<figure>
					<img src=".\Img\Von-Neumann-Arch.png" alt="Diagram of Von Neumann Architecture"></img>
					<figcaption>Figure 1 - Diagram of Von Neumann Architecture</figcaption>
				</figure>
			</section>
			<section>
				<h2>Von Neumann Bottleneck</h2>
				<p>
					As mentioned above, in order for the <abbr>ALU</abbr> to be in continuous operation, data must be fed into the <abbr>ALU</abbr> quickly. However, more often than not, this is not achieved - causing the <abbr>ALU</abbr> to spend much of the time waiting for the next instruction. This is often due to the fact that the shared memory that the Von Neumann architecture uses, also limits <a href="">throughput</a> (data transfer rate) between the memory and the <abbr>CPU</abbr> as instructions and data cannot be accessed at the same time. This problem becomes more severe with fast operations on large amounts of data.
				</p>
				<p>
					There are 2 main ways to limit the Von Neumann Bottleneck, they are:
					<ol>
						<li>Alternative Architectures (e.g. Harvard Architecture)</li>
						<li>Minimising use of main memory</li>
						<ul>
							<li>Caching</li>
							<li>Internal Registers</li>
						</ul>
					</ol>
				</p>
				<h3>Harvard Architecture</h3>
				<p>
					Unlike the Von Neumann Architecture, Harvard Architecture does use seperate memory stores for instructions and data. This allows the <abbr>ALU</abbr> and Control Unit to access the memory simultaneously. This increase in performance is offset by a decrease in flexibility (memory is also smaller). Therefore it is only used in specialised hardware like microcontrollers and embedded systems (like an Arduino), or in processors that are programmed to utilise the simultaneous access to memory. Harvard Architecture has been modified over the years to improve its performance. These include providing a data pathway between the 2 storages, resulting in only 1 loading mechanism from store to <abbr>CPU</abbr> and having seperate caching for data and instructions, but a single store - allowing a processor to function in Harvard or Von Neumann mode.
				</p>
				<figure>
					<img src=".\Img\Harvard-Arch.png" alt="Diagram of Harvard Architecture"></img>
					<figcaption>Figure 2 - Diagram of Harvard Architecture</figcaption>
				</figure>
				<h3>Caching</h3>
				<p>
					A cache is a small data store that is a duplicate of some parts of the main memory, but capable of more rapid access. Cache is also known as <abbr title="Static RAM">SRAM</abbr> and the main memory is known as <abbr title="Dynamic RAM">DRAM</abbr>. There are 2 types of cache, L1 and L2. L1 cache is very close to the processor and provides very fast access, however it is rather small. L2 cache is just off the chip, and is slightly slower than L1, but has a larger store.
				</p>
				<h3>Registers</h3>
				<p>
					Registers provide the fastest access to the <abbr>CPU</abbr> and are used to store values that are used in ongoing computations. Registers are stored on the <abbr>CPU</abbr>. Most programming languages (exceptions include C and Assembly) do not allow direct access to the register.
				</p>
			</section>
		</main>
	</body>
</html>